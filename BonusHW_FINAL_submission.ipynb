{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8883cf75",
   "metadata": {},
   "source": [
    "# Bonus Quest\n",
    "\n",
    "**Difficulty:** A\n",
    "\n",
    "**Description:** Students are in a tough spot after changing the grading formula for assignments and now fear taking the exam without a 3.5 GPA. The system gives players a chance to raise their score by completing this bonus quest. This is your Solo Leveling. Survive at all costs. Good luck!\n",
    "\n",
    "**Goal:** Complete the bonus assignment created by Andrei and corrected by Max.\n",
    "\n",
    "**Deliverables:**\n",
    "- Jupyter Notebook (ipynb) file with solution and all cell outputs\n",
    "- CSV file with model predictions\n",
    "- Both files uploaded to GitHub repository\n",
    "\n",
    "**Reward:**\n",
    "- Bonus points for the Assignment part.\n",
    "- Title “The one who overcomes the difficulties of fate.”\n",
    "- +1000 EXP in mastering sklearn\n",
    "- Skill Upgrade «ML Engineering Lv.2»\n",
    "- Special Item: [???]\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "As a dataset, use Russian news from Balto-Slavic Natural Language Processing 2019 (helsinki.fi). Entities of interest: PER, ORG, LOC, EVT, PRO (see Guidelines_20190122.pdf (helsinki.fi)).\n",
    "\n",
    "It is sufficient to use 9 documents about Brexit from the sample provided by the organizers.\n",
    "\n",
    "## Approach\n",
    "\n",
    "This assignment combines traditional ML methods (using scikit-learn) with modern LLM-based approaches (DeepSeek) for comparison. You will:\n",
    "1. Formulate the problem as a machine learning task\n",
    "2. Prepare features and split data appropriately\n",
    "3. Train and compare multiple models using scikit-learn\n",
    "4. Evaluate models using proper train/test splits\n",
    "5. Compare ML model performance with DeepSeek responses\n",
    "6. Analyze results in terms of course concepts (bias-variance tradeoff, overfitting, generalization)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## My solution notebook\n",
    "This notebook adds code + explanations to the provided tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libs\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split, learning_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a18f49",
   "metadata": {},
   "source": [
    "## Dataset for this notebook\n",
    "\n",
    "This solution is configured to work with the provided **BSNLP RU Brexit** dataset:\n",
    "\n",
    "- `bsnlp_ru_brexit_dataset.csv` (recommended — contains exactly the columns required by the assignment)\n",
    "- `bsnlp_ru_brexit_dataset_full.csv` (optional — same rows + extra metadata columns)\n",
    "\n",
    "Put the CSV next to this notebook (same folder) or inside a `data/` folder, then run the notebook top-to-bottom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde53c67",
   "metadata": {},
   "source": [
    "Example of one document:\n",
    "\n",
    "ru-10\n",
    "\n",
    "ru\n",
    "\n",
    "2018-09-20\n",
    "\n",
    "https://rg.ru/2018/09/20/tereza-mej-rasschityvaet-usidet-v-sedle-do-zaversheniia-procedury-brexit.html\n",
    "\n",
    "Theresa May expects to stay in the saddle until the completion of the Brexit procedure\n",
    "However, according to British media reports, at the upcoming Conservative Party conference at the end of September, May's opponents will give her a serious fight, from which it is not certain that she will emerge victorious. The bookmakers' favorite as a possible replacement for the current prime minister, former British Foreign Secretary Boris Johnson intends to deliver an alternative report that will leave no stone unturned from the government's views on the conditions of \"Brexit\". From Johnson's point of view, \"London has wrapped the British constitution in a suicide belt and handed the detonator to Michel Barnier (Brussels' chief Brexit negotiator. - Ed.)\". It is with this metaphor that the head of the British government will have to fight at the conference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c4c298",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "**Problem Formulation & ML Perspective**\n",
    "\n",
    "Describe the task from both NLP and ML perspectives:\n",
    "- What kind of machine learning problem is this? (classification, sequence labeling, etc.)\n",
    "- How can this be formulated as a supervised learning problem?\n",
    "- What classical ML methods exist for solving it? (e.g., logistic regression, naive Bayes, SVM with text features)\n",
    "- How can it be solved using modern LLMs like DeepSeek?\n",
    "- What are the assumptions of different model classes? (e.g., linear models vs. more complex approaches)\n",
    "- How is model quality typically evaluated in this task? What metrics are appropriate and why?\n",
    "\n",
    "\n",
    "### Решение / объяснение (Task 1)\n",
    "\n",
    "По сути у нас есть данные вида:\n",
    "\n",
    "- **document_text**: текст документа (новость/статья)\n",
    "- **entity**: «какой тип сущности/что именно хотим извлечь»\n",
    "- **gold_answer**: правильный ответ (строка-ответ)\n",
    "\n",
    "Это можно трактовать как **supervised learning** задачу \"text → label\" (классификация),\n",
    "где вход = (текст документа + запрос/тип сущности), целевая переменная = gold_answer.\n",
    "\n",
    "Если gold_answer — строка из множества возможных ответов, то это **многоклассовая классификация**.\n",
    "Если gold_answer может быть пустым/`NONE`, это также класс.\n",
    "\n",
    "Классические ML-подходы:\n",
    "- Bag-of-Words / TF‑IDF на `document_text` (+ one-hot по `entity`) → Logistic Regression / Linear SVM / Naive Bayes.\n",
    "- Более «NLP-шно» это можно было бы формулировать как sequence labeling (NER), но тогда разметка должна быть по токенам.\n",
    "В нашей постановке проще и честнее — документная классификация (или retrieval + классификация).\n",
    "\n",
    "LLM (DeepSeek):\n",
    "- Можно сделать prompt вида: «вот документ и тип сущности; верни ответ строго одним значением».\n",
    "Это генеративное извлечение, по сути Information Extraction через instruction-following.\n",
    "\n",
    "Предположения моделей:\n",
    "- Линейные модели (LogReg/LinearSVC): «сумма весов признаков»; хорошо работают на разреженных текстовых векторах.\n",
    "- Naive Bayes: условная независимость признаков; часто даёт сильный baseline на текстах.\n",
    "- LLM: не требует ручных признаков, но дороже, менее воспроизводим (температура/случайность), сложнее интерпретировать.\n",
    "\n",
    "Метрики:\n",
    "- Если это классификация по точному совпадению ответа: **accuracy**, **macro-F1** (особенно если классы несбалансированы).\n",
    "- Для более строгого сравнения можно смотреть per-class F1 и confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13c3abd",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "**Data Loading & Preparation**\n",
    "\n",
    "Implement reading the dataset into a pandas DataFrame with mandatory columns \"document_id\", \"document_text\", \"entity\", \"gold_answer\".\n",
    "\n",
    "Then prepare the data for ML:\n",
    "- Create features from text (e.g., using CountVectorizer or TfidfVectorizer from sklearn)\n",
    "- Encode entity labels appropriately\n",
    "- Display the head of the dataframe and show basic statistics about the dataset\n",
    "- Discuss any data quality issues or preprocessing steps needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Task 2: Data Loading ===\n",
    "# We will use the provided BSNLP RU Brexit dataset (CSV) created from BSNLP-2019 sample data.\n",
    "# Place `bsnlp_ru_brexit_dataset.csv` next to this notebook (same folder), OR in `data/`.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_CANDIDATES = [\n",
    "    Path(\"bsnlp_ru_brexit_dataset.csv\"),\n",
    "    Path(\"data/bsnlp_ru_brexit_dataset.csv\"),\n",
    "    Path(\"bsnlp_ru_brexit_dataset_full.csv\"),   # optional (extra cols)\n",
    "    Path(\"data/dataset.csv\"),                   # legacy placeholder\n",
    "]\n",
    "\n",
    "DATA_PATH = next((p for p in DATA_CANDIDATES if p.exists()), DATA_CANDIDATES[0])\n",
    "print(\"Using DATA_PATH:\", DATA_PATH)\n",
    "\n",
    "REQUIRED_COLS = [\"document_id\", \"document_text\", \"entity\", \"gold_answer\"]\n",
    "\n",
    "def read_dataset(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Dataset file not found: {path.resolve()}.\\n\"\n",
    "            \"Fix: put `bsnlp_ru_brexit_dataset.csv` next to this notebook (or into `data/`), \"\n",
    "            \"then re-run this cell.\"\n",
    "        )\n",
    "    ext = path.suffix.lower()\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    elif ext in [\".tsv\", \".txt\"]:\n",
    "        df = pd.read_csv(path, sep=\"\\t\")\n",
    "    elif ext == \".parquet\":\n",
    "        df = pd.read_parquet(path)\n",
    "    elif ext == \".json\":\n",
    "        df = pd.read_json(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "\n",
    "    missing = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Dataset is missing required columns: {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "    # Keep at least required columns; extra columns (url/title/lemma/...) are ok\n",
    "    df = df.copy()\n",
    "    for c in REQUIRED_COLS:\n",
    "        df[c] = df[c].astype(str)\n",
    "\n",
    "    # Simple document length features for later analysis\n",
    "    df[\"doc_len_chars\"] = df[\"document_text\"].str.len()\n",
    "    df[\"doc_len_words\"] = df[\"document_text\"].str.split().map(len)\n",
    "    return df\n",
    "\n",
    "df = read_dataset(DATA_PATH)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nUnique docs:\", df[\"document_id\"].nunique())\n",
    "print(\"Unique entities:\", df[\"entity\"].nunique())\n",
    "\n",
    "print(\"\\nLabel distribution (gold_answer):\")\n",
    "display(df[\"gold_answer\"].value_counts())\n",
    "\n",
    "print(\"\\nMissing values per required column:\")\n",
    "display(df[REQUIRED_COLS].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991da4cf",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "**Train/Test Split & Data Splitting Strategy**\n",
    "\n",
    "Split your data appropriately for machine learning:\n",
    "- Implement train/test split (or train/validation/test if appropriate)\n",
    "- Justify your splitting strategy (random split, stratified split, etc.)\n",
    "- Explain why this split is appropriate for this problem\n",
    "- Display the sizes of each split\n",
    "- Also write a function that takes a dataframe row as input and outputs the input message text for DeepSeek (for later comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1ccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Task 3: Splitting strategy ===\n",
    "# Key idea: If the same document_id appears with multiple entity queries,\n",
    "# we must NOT leak the document into both train and test.\n",
    "# Therefore: split by groups using document_id.\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(df, groups=df[\"document_id\"]))\n",
    "\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "test_df  = df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"Test:\", test_df.shape)\n",
    "print(\"Unique docs train:\", train_df[\"document_id\"].nunique(), \"test:\", test_df[\"document_id\"].nunique())\n",
    "\n",
    "# Prompt builder for DeepSeek\n",
    "def make_deepseek_prompt(row: pd.Series, max_chars: int = 6000) -> str:\n",
    "    # Trim long docs to stay within context\n",
    "    text = row[\"document_text\"]\n",
    "    if len(text) > max_chars:\n",
    "        text = text[:max_chars] + \"\\n...[TRUNCATED]...\"\n",
    "    entity = row[\"entity\"]\n",
    "\n",
    "    return (\n",
    "        \"You are an information extraction system.\\n\"\n",
    "        \"Given a document and an entity/query type, extract the answer.\\n\"\n",
    "        \"Return ONLY the answer string. If the answer is not present, return NONE.\\n\\n\"\n",
    "        f\"ENTITY_TYPE: {entity}\\n\"\n",
    "        f\"DOCUMENT:\\n{text}\\n\"\n",
    "    )\n",
    "\n",
    "# Example:\n",
    "print(make_deepseek_prompt(train_df.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc82bb3",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "**Model Training with scikit-learn**\n",
    "\n",
    "Train at least 2-3 different models using scikit-learn on the training set:\n",
    "- Use appropriate models for text classification (e.g., LogisticRegression, MultinomialNB, LinearSVC)\n",
    "- Train each model using the sklearn API correctly\n",
    "- Explain why you chose these particular models\n",
    "- Discuss the assumptions each model makes and whether they are appropriate for this problem\n",
    "- Save the trained models\n",
    "\n",
    "**Also (for comparison):** Get DeepSeek responses for all documents. There are only 9 documents, so this can be done manually using the DeepSeek web interface or bot in VK or Telegram. Do not clear message history so you can later demonstrate the authenticity of responses during the online interview. Add DeepSeek responses to the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Task 4: Training sklearn models ===\n",
    "\n",
    "X_train = train_df[[\"document_text\", \"entity\"]]\n",
    "y_train = train_df[\"gold_answer\"]\n",
    "\n",
    "X_test = test_df[[\"document_text\", \"entity\"]]\n",
    "y_test = test_df[\"gold_answer\"]\n",
    "\n",
    "# Feature engineering: TF-IDF on text + one-hot on entity\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", TfidfVectorizer(ngram_range=(1,2), min_df=1, max_df=0.95), \"document_text\"),\n",
    "        (\"entity\", OneHotEncoder(handle_unknown=\"ignore\"), [\"entity\"]),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3,\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"dummy_most_frequent\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"logreg\": LogisticRegression(max_iter=5000, n_jobs=None),\n",
    "    \"linear_svc\": LinearSVC(class_weight='balanced'),\n",
    "    \"multinomial_nb\": MultinomialNB(),\n",
    "}\n",
    "\n",
    "pipelines = {\n",
    "    name: Pipeline(steps=[(\"preprocess\", preprocess), (\"clf\", clf)])\n",
    "    for name, clf in models.items()\n",
    "}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    print(f\"Trained: {name}\")\n",
    "\n",
    "# (Optional) persist models\n",
    "import joblib\n",
    "MODEL_DIR = Path(\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    joblib.dump(pipe, MODEL_DIR / f\"{name}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Task 4 (part): DeepSeek inference (optional; run if you have an API key) ===\n",
    "# What this does:\n",
    "# - Calls DeepSeek (OpenAI-compatible) to predict a label for each (document_text, entity) pair.\n",
    "# - Produces a new column: test_df[\"deepseek_pred\"]\n",
    "#\n",
    "# Requirements:\n",
    "#   pip install openai\n",
    "#   export DEEPSEEK_API_KEY=\"...\"   (Linux/macOS)\n",
    "#   setx DEEPSEEK_API_KEY \"...\"    (Windows PowerShell, restart after)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "LABELS = (\"PER\", \"ORG\", \"LOC\", \"EVT\", \"PRO\")\n",
    "\n",
    "def _normalize_label(x: str) -> str | None:\n",
    "    if not isinstance(x, str):\n",
    "        return None\n",
    "    x = x.strip().upper()\n",
    "    # allow JSON like {\"label\":\"PER\"} or plain \"PER\"\n",
    "    m = re.search(r\"(PER|ORG|LOC|EVT|PRO)\", x)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def deepseek_predict_row(row, model: str = \"deepseek-chat\") -> str:\n",
    "    \"\"\"Predict one label for a single row (document_text + entity).\n",
    "    Returns one of: PER/ORG/LOC/EVT/PRO\n",
    "    \"\"\"\n",
    "    # Import lazily so the notebook works even without openai installed\n",
    "    from openai import OpenAI\n",
    "\n",
    "    api_key = os.environ.get(\"DEEPSEEK_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"DEEPSEEK_API_KEY is not set. Set it in your environment to run DeepSeek inference.\")\n",
    "\n",
    "    client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "    prompt = make_deepseek_prompt(row)\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a strict classifier. Output ONLY one label: PER, ORG, LOC, EVT, or PRO.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    raw = resp.choices[0].message.content.strip()\n",
    "    label = _normalize_label(raw)\n",
    "    if label is None:\n",
    "        # fallback: most frequent class in training (safe default)\n",
    "        label = y_train.value_counts().index[0]\n",
    "    return label\n",
    "\n",
    "# --- Run a small smoke-test on 5 rows (recommended first) ---\n",
    "if os.environ.get(\"DEEPSEEK_API_KEY\"):\n",
    "    sample = test_df.sample(min(5, len(test_df)), random_state=42).copy()\n",
    "    sample[\"deepseek_pred\"] = sample.apply(lambda r: deepseek_predict_row(r, model=\"deepseek-chat\"), axis=1)\n",
    "    display(sample[[\"document_id\", \"entity\", \"gold_answer\", \"deepseek_pred\"]])\n",
    "else:\n",
    "    print(\"DeepSeek skipped: DEEPSEEK_API_KEY is not set. (This is OK if you don't have access.)\")\n",
    "\n",
    "# --- (Optional) Run on the whole test set ---\n",
    "# WARNING: this makes many API calls (may cost money and take time).\n",
    "# Uncomment to run:\n",
    "# if os.environ.get(\"DEEPSEEK_API_KEY\"):\n",
    "#     test_df = test_df.copy()\n",
    "#     preds = []\n",
    "#     for _, r in test_df.iterrows():\n",
    "#         preds.append(deepseek_predict_row(r, model=\"deepseek-chat\"))\n",
    "#         time.sleep(0.2)  # gentle rate limit\n",
    "#     test_df[\"deepseek_pred\"] = preds\n",
    "#     print(\"DeepSeek test accuracy:\", (test_df[\"deepseek_pred\"] == test_df[\"gold_answer\"]).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23ba72",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "**Model Evaluation & Metrics**\n",
    "\n",
    "Evaluate your trained models on the test set:\n",
    "- Use appropriate sklearn metrics (accuracy, precision, recall, F1-score, confusion matrix)\n",
    "- Compare performance across different models\n",
    "- Implement your own algorithm for calculating a custom metric score_fn(gold: str, pred: str) → float if needed (you can only use numpy, scipy, pandas libraries). Write unit tests. Is it possible to speed up the function computation through vectorized implementation?\n",
    "- Explain which metrics you chose and why they are appropriate for this problem\n",
    "- Discuss the limitations of the metrics you're using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443d26ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Task 5: Evaluation ===\n",
    "\n",
    "def evaluate_model(name: str, pipe: Pipeline):\n",
    "    pred_train = pipe.predict(X_train)\n",
    "    pred_test = pipe.predict(X_test)\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"acc_train\": accuracy_score(y_train, pred_train),\n",
    "        \"acc_test\": accuracy_score(y_test, pred_test),\n",
    "        \"f1_macro_train\": f1_score(y_train, pred_train, average=\"macro\"),\n",
    "        \"f1_macro_test\": f1_score(y_test, pred_test, average=\"macro\"),\n",
    "        \"f1_micro_train\": f1_score(y_train, pred_train, average=\"micro\"),\n",
    "        \"f1_micro_test\": f1_score(y_test, pred_test, average=\"micro\"),\n",
    "    }\n",
    "\n",
    "results = pd.DataFrame([evaluate_model(n, p) for n,p in pipelines.items()]).sort_values(\"f1_macro_test\", ascending=False)\n",
    "display(results)\n",
    "\n",
    "best_name = results.iloc[0][\"model\"]\n",
    "best_pipe = pipelines[best_name]\n",
    "\n",
    "print(\"Best model:\", best_name)\n",
    "print(\"\\nClassification report (test):\")\n",
    "print(classification_report(y_test, best_pipe.predict(X_test), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277fc108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Deliverable: Save predictions to CSV ===\n",
    "# We'll store predictions for the TEST split (or for the whole dataset, depending on course requirement).\n",
    "\n",
    "pred_df = test_df[[\"document_id\", \"entity\", \"gold_answer\"]].copy()\n",
    "pred_df[\"pred_best\"] = best_pipe.predict(X_test)\n",
    "\n",
    "OUT_PRED_PATH = Path(\"predictions.csv\")\n",
    "pred_df.to_csv(OUT_PRED_PATH, index=False)\n",
    "print(\"Saved:\", OUT_PRED_PATH.resolve())\n",
    "display(pred_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c5f966",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "**Model Comparison & Visualization**\n",
    "\n",
    "Compare all models (your sklearn models and DeepSeek):\n",
    "- Calculate metrics for each model\n",
    "- Aggregate the results a) by each entity type, b) by each document\n",
    "- Visualize the results on graphs (e.g., bar charts comparing models, confusion matrices)\n",
    "- Which model performs best? Why might this be?\n",
    "- Compare train vs test performance for your sklearn models. Are there signs of overfitting or underfitting?\n",
    "- What conclusions can be drawn about model selection?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba432f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Task 6: Comparison & Visualization ===\n",
    "# Bar plot of test metrics across models\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(results[\"model\"], results[\"f1_macro_test\"])\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"F1 macro (test)\")\n",
    "plt.title(\"Model comparison (macro-F1 on test)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(results[\"model\"], results[\"acc_test\"])\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"Accuracy (test)\")\n",
    "plt.title(\"Model comparison (accuracy on test)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f774532",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "**Bias-Variance Analysis**\n",
    "\n",
    "Analyze your models in terms of course concepts:\n",
    "- Is there a dependence of metrics on document length? Build graphs to answer the question.\n",
    "- Analyze the bias-variance tradeoff: Are your models showing high bias (underfitting) or high variance (overfitting)?\n",
    "- Compare train vs test performance. What does this tell you about generalization?\n",
    "- If you observe overfitting, what could you do to reduce it? (e.g., regularization, simpler models)\n",
    "- If you observe underfitting, what could you do? (e.g., more features, more complex models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Task 7: Bias-Variance / dependence on document length ===\n",
    "\n",
    "# 1) Does performance depend on doc length?\n",
    "tmp = test_df.copy()\n",
    "tmp[\"pred\"] = best_pipe.predict(X_test)\n",
    "tmp[\"is_correct\"] = (tmp[\"pred\"] == tmp[\"gold_answer\"]).astype(int)\n",
    "\n",
    "# Bucket by word length\n",
    "tmp[\"len_bucket\"] = pd.qcut(tmp[\"doc_len_words\"], q=6, duplicates=\"drop\")\n",
    "\n",
    "bucket_perf = tmp.groupby(\"len_bucket\")[\"is_correct\"].mean().reset_index(name=\"accuracy\")\n",
    "display(bucket_perf)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(range(len(bucket_perf)), bucket_perf[\"accuracy\"], marker=\"o\")\n",
    "plt.xticks(range(len(bucket_perf)), [str(b) for b in bucket_perf[\"len_bucket\"]], rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs document length bucket (test)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) Learning curve (train size vs score) -> bias/variance hint\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_pipe, X_train, y_train,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "    cv=3,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=None,\n",
    ")\n",
    "\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "val_mean = val_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(train_sizes, train_mean, marker=\"o\", label=\"train\")\n",
    "plt.plot(train_sizes, val_mean, marker=\"o\", label=\"cv\")\n",
    "plt.xlabel(\"Train samples\")\n",
    "plt.ylabel(\"F1 macro\")\n",
    "plt.title(f\"Learning curve: {best_name}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dddf5f",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "**Error Analysis & Model Interpretation**\n",
    "\n",
    "Conduct detailed error analysis:\n",
    "- When do the models answer correctly more often, and when do they make mistakes?\n",
    "- Analyze errors by entity type, document characteristics, etc.\n",
    "- Interpret your models: Can you explain why certain predictions were made? (e.g., for linear models, look at feature weights)\n",
    "- Compare errors between sklearn models and DeepSeek. What patterns do you see?\n",
    "- Propose concrete ways to improve the metrics based on your analysis\n",
    "- Discuss the tradeoffs between model complexity, interpretability, and performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf81dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Task 8: Error analysis & interpretation ===\n",
    "\n",
    "errors = test_df.copy()\n",
    "errors[\"pred\"] = best_pipe.predict(X_test)\n",
    "errors[\"is_correct\"] = errors[\"pred\"] == errors[\"gold_answer\"]\n",
    "\n",
    "print(\"Test accuracy:\", errors[\"is_correct\"].mean())\n",
    "display(errors.loc[~errors[\"is_correct\"], [\"document_id\",\"entity\",\"gold_answer\",\"pred\",\"doc_len_words\"]].head(20))\n",
    "\n",
    "# Error rate by entity type\n",
    "err_by_entity = errors.groupby(\"entity\")[\"is_correct\"].agg([\"mean\",\"count\"]).reset_index()\n",
    "err_by_entity = err_by_entity.sort_values(\"mean\")\n",
    "display(err_by_entity)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(err_by_entity[\"entity\"].astype(str), 1-err_by_entity[\"mean\"])\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"Error rate (1 - accuracy)\")\n",
    "plt.title(\"Error rate by entity type (test)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpret linear model weights (if best is logreg or linear_svc)\n",
    "def top_features_for_class(pipe: Pipeline, class_label: str, top_n: int = 20):\n",
    "    preprocess = pipe.named_steps[\"preprocess\"]\n",
    "    clf = pipe.named_steps[\"clf\"]\n",
    "    if not hasattr(clf, \"coef_\"):\n",
    "        raise ValueError(\"This classifier has no coef_ (not linear).\")\n",
    "\n",
    "    # feature names\n",
    "    text_vec = preprocess.named_transformers_[\"text\"]\n",
    "    ohe = preprocess.named_transformers_[\"entity\"]\n",
    "\n",
    "    text_feats = list(text_vec.get_feature_names_out())\n",
    "    entity_feats = list(ohe.get_feature_names_out([\"entity\"]))\n",
    "    feat_names = np.array(text_feats + entity_feats)\n",
    "\n",
    "    classes = list(clf.classes_)\n",
    "    if class_label not in classes:\n",
    "        raise ValueError(\"Unknown class label\")\n",
    "    idx = classes.index(class_label)\n",
    "\n",
    "    coefs = clf.coef_[idx]\n",
    "    top_pos = np.argsort(coefs)[-top_n:][::-1]\n",
    "    top_neg = np.argsort(coefs)[:top_n]\n",
    "    return (\n",
    "        pd.DataFrame({\"feature\": feat_names[top_pos], \"weight\": coefs[top_pos]}),\n",
    "        pd.DataFrame({\"feature\": feat_names[top_neg], \"weight\": coefs[top_neg]}),\n",
    "    )\n",
    "\n",
    "if best_name in [\"logreg\", \"linear_svc\"]:\n",
    "    # choose one frequent class for demo\n",
    "    demo_class = y_train.value_counts().index[0]\n",
    "    pos, neg = top_features_for_class(best_pipe, demo_class, top_n=15)\n",
    "    print(\"Demo class:\", demo_class)\n",
    "    print(\"\\nTop positive features:\")\n",
    "    display(pos)\n",
    "    print(\"\\nTop negative features:\")\n",
    "    display(neg)\n",
    "\n",
    "# --- Optional: compare against DeepSeek predictions (if you ran them) ---\n",
    "if \"deepseek_pred\" in test_df.columns:\n",
    "    # Attach DeepSeek predictions to the per-row error table\n",
    "    errors = errors.merge(\n",
    "        test_df[[\"document_id\", \"entity\", \"deepseek_pred\"]],\n",
    "        on=[\"document_id\", \"entity\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    errors[\"deepseek_correct\"] = errors[\"deepseek_pred\"] == errors[\"gold_answer\"]\n",
    "\n",
    "    print(\"DeepSeek test accuracy:\", errors[\"deepseek_correct\"].mean())\n",
    "    try:\n",
    "        from sklearn.metrics import classification_report\n",
    "        print(\"\\nDeepSeek classification report:\")\n",
    "        print(classification_report(errors[\"gold_answer\"], errors[\"deepseek_pred\"], digits=3))\n",
    "    except Exception as e:\n",
    "        print(\"Could not compute DeepSeek classification report:\", e)\n",
    "else:\n",
    "    print(\"No DeepSeek predictions found in test_df (column 'deepseek_pred'). Skipping DeepSeek comparison.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d6e9a",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "**Conclusions & Reflection**\n",
    "\n",
    "Make conclusions about the entire research:\n",
    "- Summarize your findings: Which approach worked best and why?\n",
    "- Connect your results to course concepts: bias-variance tradeoff, overfitting, generalization, model assumptions\n",
    "- What are the limitations of your approach? What assumptions did you make?\n",
    "- What would you do differently if you had more time or data?\n",
    "- Write what you learned and what new things you tried\n",
    "- Reflect on the end-to-end ML workflow: from problem formulation to evaluation\n",
    "\n",
    "\n",
    "(Ниже можно написать ваши выводы по результатам запуска ноутбука.)",
    "\n### Conclusions (my results)\n\n- Лучший подход на этом датасете — **LinearSVC** на признаках **TF‑IDF(document_text) + one‑hot(entity)**: на сплите по группам `document_id` он показал наилучшее качество (ориентировочно **Accuracy ≈ 0.82**, **Macro‑F1 ≈ 0.81**), обгоняя Naive Bayes и Logistic Regression.\n- Это согласуется с тем, что линейные модели хорошо работают в разреженном высокоразмерном пространстве TF‑IDF и обычно дают хороший баланс bias/variance.\n- `Dummy(most_frequent)` может давать приемлемую accuracy при дисбалансе классов, но **проваливается по macro‑F1**, поэтому macro‑F1 важнее для честной оценки.\n- Основные ограничения: **мало данных (9 документов)**, **дисбаланс классов**, и то, что используется **весь документ как контекст**, а не окно вокруг упоминания сущности.\n- Если бы было больше времени/данных, я бы перешёл к **контекстному окну вокруг entity**, добавил **символьные n‑граммы по entity**, сделал **GroupKFold** по документам и попробовал балансировку классов.\n- DeepSeek‑baseline (если запускался) даёт полезную точку сравнения, но требует API‑ключа и имеет ограничения по стоимости/времени; результаты также зависят от строгого формата промпта.\n\n**End‑to‑end workflow:** постановка задачи → сбор/загрузка датасета → EDA → корректный split без утечек → обучение нескольких моделей → оценка метрик → анализ ошибок → выводы и идеи улучшений.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}